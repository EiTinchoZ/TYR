# ğŸ¤ GUIÃ“N PRESENTACIÃ“N TYR - LENGUAJES DE PROGRAMACIÃ“N PARA LA IA
## Proyecto Final - Arquitectura y Desarrollo
**Estudiante:** MartÃ­n Bundy
**Materia:** Lenguajes de ProgramaciÃ³n para la IA
**Fecha:** Diciembre 2025
**DuraciÃ³n:** 15-20 minutos

---

# ğŸ“‹ ESTRUCTURA DE LA PRESENTACIÃ“N

1. [IntroducciÃ³n (2 min)](#1-introducciÃ³n-2-min)
2. [Arquitectura del Sistema (4 min)](#2-arquitectura-del-sistema-4-min)
3. [Lenguajes y Ecosistemas (5 min)](#3-lenguajes-y-ecosistemas-5-min)
4. [Decisiones TÃ©cnicas y Patrones (4 min)](#4-decisiones-tÃ©cnicas-y-patrones-4-min)
5. [DemostraciÃ³n TÃ©cnica (3 min)](#5-demostraciÃ³n-tÃ©cnica-3-min)
6. [IntegraciÃ³n y Deploy (2 min)](#6-integraciÃ³n-y-deploy-2-min)
7. [Conclusiones (1 min)](#7-conclusiones-1-min)

---

# 1. INTRODUCCIÃ“N (â‰ˆ2 MIN)

Buenos dÃ­as/tardes. Soy MartÃ­n Bundy y hoy presento **TYR**, un chatbot inteligente para el ITSE, desde la perspectiva de **Lenguajes de ProgramaciÃ³n para la IA**.

En PLN demostrÃ© las tÃ©cnicas de procesamiento de lenguaje natural. Hoy quiero mostrarles **cÃ³mo se programa y construye** un sistema de IA completo en producciÃ³n.

## Â¿Por quÃ© este proyecto?

DecidÃ­ construir TYR como mi primer proyecto real individual, donde puse a prueba:
- **Arquitectura full-stack** con mÃºltiples lenguajes
- **IntegraciÃ³n de modelos de IA** en producciÃ³n
- **DiseÃ±o de APIs** y sistemas escalables
- **ProgramaciÃ³n orientada a objetos** y funcional
- **Testing automatizado** y buenas prÃ¡cticas

TYR (dios nÃ³rdico de la justicia) simboliza **precisiÃ³n y confiabilidad tÃ©cnica**.

## Lo que verÃ¡n hoy:

1. CÃ³mo integrar **Python + TypeScript** en un sistema de IA
2. Patrones de diseÃ±o para aplicaciones de machine learning
3. Arquitectura API REST con FastAPI
4. Manejo de modelos transformer en producciÃ³n
5. Testing y validaciÃ³n automatizada

---

# 2. ARQUITECTURA DEL SISTEMA (â‰ˆ4 MIN)

## Stack TecnolÃ³gico Multi-Lenguaje

TYR estÃ¡ construido con **3 lenguajes principales**, cada uno elegido estratÃ©gicamente:

### **Python 3.14** (Backend + IA)
```
â”œâ”€â”€ tyr_chatbot.py       (Clase principal - 1,400 lÃ­neas)
â”œâ”€â”€ ner_module.py        (NER personalizado - 391 lÃ­neas)
â”œâ”€â”€ main.py              (API FastAPI - 150 lÃ­neas)
â””â”€â”€ tyr_simple.py        (Wrapper - 80 lÃ­neas)
```

**Â¿Por quÃ© Python?**
- Ecosistema de IA/ML mÃ¡s maduro (PyTorch, Transformers, VADER)
- Sintaxis clara para algoritmos complejos
- LibrerÃ­as cientÃ­ficas optimizadas (NumPy, etc.)
- RÃ¡pido desarrollo de prototipos

### **TypeScript** (Frontend)
```
â”œâ”€â”€ TYRChat.tsx          (Componente React - 1,100 lÃ­neas)
â”œâ”€â”€ mockResponses.ts     (Sistema demo - 140 lÃ­neas)
â””â”€â”€ Interfaces           (Type safety)
```

**Â¿Por quÃ© TypeScript sobre JavaScript?**
- **Type safety**: DetecciÃ³n de errores en tiempo de compilaciÃ³n
- **Interfaces explÃ­citas**: Contratos claros entre frontend-backend
- **IntelliSense**: Autocompletado y navegaciÃ³n de cÃ³digo
- **Refactoring seguro**: Cambios sin romper dependencias

### **JSON** (ConfiguraciÃ³n y Datos)
```
â”œâ”€â”€ config.json          (ConfiguraciÃ³n modelo BERT)
â”œâ”€â”€ label_map.json       (Mapeo de intenciones)
â”œâ”€â”€ carreras_itse.json   (Base de conocimiento - 16 carreras)
â””â”€â”€ respuestas_base.json (Respuestas por intenciÃ³n)
```

**Â¿Por quÃ© JSON?**
- Formato universal entre lenguajes
- FÃ¡cil versionado y modificaciÃ³n
- Legible para humanos y mÃ¡quinas

---

## Arquitectura de 3 Capas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CAPA DE PRESENTACIÃ“N                â”‚
â”‚  React + TypeScript + Tailwind CSS          â”‚
â”‚  - Componentes funcionales con hooks        â”‚
â”‚  - Estado con useState/useEffect            â”‚
â”‚  - Type-safe interfaces                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ HTTP/JSON (REST API)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CAPA DE APLICACIÃ“N                  â”‚
â”‚  FastAPI + Pydantic + Uvicorn               â”‚
â”‚  - ValidaciÃ³n automÃ¡tica con Pydantic       â”‚
â”‚  - DocumentaciÃ³n auto-generada (OpenAPI)    â”‚
â”‚  - Async/await para concurrencia            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ OOP + FunciÃ³n calls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CAPA DE LÃ“GICA DE IA                â”‚
â”‚  PyTorch + Transformers + VADER + NER       â”‚
â”‚  - Clase TYR (OOP)                          â”‚
â”‚  - MÃ³dulos especializados                   â”‚
â”‚  - GPU/CPU abstraction                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas de esta arquitectura:**
- **SeparaciÃ³n de responsabilidades**: Cada capa tiene un propÃ³sito claro
- **Escalabilidad**: Puedo escalar frontend y backend independientemente
- **Mantenibilidad**: Cambios en UI no afectan la lÃ³gica de IA
- **Testeable**: Cada capa se puede testear aisladamente

---

## Flujo de Datos End-to-End

1. **Usuario escribe** en el frontend (TypeScript)
2. **Fetch API** envÃ­a POST a `/chat` (HTTP/JSON)
3. **FastAPI recibe** y valida con Pydantic models
4. **tyr_simple.py** procesa la consulta:
   - Instancia clase `TYR`
   - Ejecuta `procesar_consulta()`
5. **tyr_chatbot.py** ejecuta pipeline:
   - NormalizaciÃ³n de texto
   - TokenizaciÃ³n con BERT
   - ClasificaciÃ³n con PyTorch
   - NER con regex personalizado
   - AnÃ¡lisis VADER
6. **Respuesta JSON** retorna con estructura:
   ```json
   {
     "respuesta": "...",
     "intencion": "informacion_carreras",
     "confianza": 0.989,
     "sentimiento": "positivo",
     "sentimiento_compound": 0.8,
     "entidades": {...}
   }
   ```
7. **Frontend renderiza** con React + visualizaciÃ³n

---

# 3. LENGUAJES Y ECOSISTEMAS (â‰ˆ5 MIN)

## 3.1 Python - Backend y Machine Learning

### CaracterÃ­sticas del Lenguaje Usadas:

#### **ProgramaciÃ³n Orientada a Objetos**
```python
class TYR:
    """Clase principal del chatbot"""

    def __init__(self, modelo_path: str, max_length: int = 128):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.max_length = max_length
        self._cargar_modelo()
        self._cargar_respuestas_base()

    def procesar_consulta(self, mensaje: str) -> Tuple[str, Dict]:
        """Pipeline completo de procesamiento"""
        # 1. Normalizar
        mensaje_norm = self._normalizar_texto(mensaje)
        # 2. Clasificar
        intencion, confianza = self._clasificar_intencion(mensaje_norm)
        # 3. Extraer entidades
        entidades = self.ner.extraer_entidades(mensaje)
        # 4. Generar respuesta
        return self._generar_respuesta(intencion, entidades)
```

**Ventajas OOP aplicadas:**
- **EncapsulaciÃ³n**: MÃ©todos privados (`_cargar_modelo`) ocultan complejidad
- **Estado interno**: `self.modelo`, `self.tokenizer` se mantienen en memoria
- **ReutilizaciÃ³n**: Una instancia sirve para mÃºltiples consultas

#### **Type Hints (Python 3.5+)**
```python
from typing import Dict, Tuple, Optional, List

def extraer_entidades(self, texto: str) -> List[Dict[str, any]]:
    """
    Extrae entidades nombradas del texto.

    Args:
        texto: String a analizar

    Returns:
        Lista de diccionarios con entidades detectadas
    """
    entidades: List[Dict[str, any]] = []
    # ... lÃ³gica
    return entidades
```

**Beneficios:**
- DocumentaciÃ³n viva en el cÃ³digo
- DetecciÃ³n de errores con mypy
- Mejor IDE support

#### **List/Dict Comprehensions**
```python
# Filtrado eficiente de entidades
entidades_unicas = [
    e for e in entidades
    if e['tipo'] == 'CARRERA' and e['confianza'] > 0.8
]

# AgrupaciÃ³n por tipo
entidades_por_tipo = {
    tipo: [e['texto'] for e in entidades if e['tipo'] == tipo]
    for tipo in ['CARRERA', 'ORGANIZACION', 'UBICACION']
}
```

**Ventaja:** CÃ³digo mÃ¡s conciso y legible que loops tradicionales

#### **Context Managers**
```python
with torch.no_grad():
    outputs = self.modelo(**inputs)
    logits = outputs.logits
```

**Ventaja:** Manejo automÃ¡tico de recursos (aquÃ­, desactivar gradientes para inferencia)

#### **Decoradores**
```python
@torch.no_grad()
def predecir(self, texto: str) -> int:
    """PredicciÃ³n sin calcular gradientes"""
    return self.modelo(texto)
```

---

### LibrerÃ­as Clave y su Rol:

#### **PyTorch** (ML Framework)
```python
import torch
from torch import nn

# Device abstraction
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Inferencia
with torch.no_grad():
    outputs = modelo(**inputs)
    prediccion = torch.argmax(outputs.logits, dim=1).item()
```

**Por quÃ© PyTorch:**
- Pythonic: Se siente como Python nativo
- Debugging fÃ¡cil: EjecuciÃ³n eager
- Soporte GPU automÃ¡tico
- Ecosistema Transformers

#### **Transformers (Hugging Face)**
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("modelo_bert_tyr_4358")
modelo = AutoModelForSequenceClassification.from_pretrained("modelo_bert_tyr_4358")
```

**AbstracciÃ³n clave:** `Auto*` clases detectan automÃ¡ticamente la arquitectura

#### **FastAPI** (API Framework)
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field

class ChatRequest(BaseModel):
    mensaje: str = Field(..., min_length=1, max_length=500)

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    respuesta = tyr.procesar_consulta(request.mensaje)
    return ChatResponse(**respuesta)
```

**CaracterÃ­sticas usadas:**
- **Pydantic models**: ValidaciÃ³n automÃ¡tica de tipos
- **Async/await**: Concurrencia con asyncio
- **Decoradores**: Routing con `@app.post`
- **Type hints**: DocumentaciÃ³n OpenAPI automÃ¡tica

---

## 3.2 TypeScript - Frontend Type-Safe

### Ventajas sobre JavaScript:

#### **Interfaces ExplÃ­citas**
```typescript
interface Message {
  id: string;
  texto: string;
  esUsuario: boolean;
  timestamp: Date;
  intencion?: string;
  confianza?: number;
  sentimiento?: string;
  sentimiento_compound?: number;
  entidades?: {
    [key: string]: string[];
  };
}

interface ChatResponse {
  respuesta: string;
  intencion: string;
  confianza: number;
  sentimiento: string;
  sentimiento_compound: number;
  entidades?: { [key: string]: string[] };
}
```

**Ventaja:** Contrato claro con el backend. Si la API cambia, TypeScript me avisa.

#### **Generics y Type Safety**
```typescript
const [mensajes, setMensajes] = useState<Message[]>([]);
const [conversaciones, setConversaciones] = useState<Conversation[]>([]);

// TypeScript sabe que 'mensaje' es tipo Message
mensajes.map((mensaje: Message) => (
  <div key={mensaje.id}>
    {mensaje.texto}
  </div>
))
```

#### **Union Types**
```typescript
type SentimentType = "positivo" | "negativo" | "neutro";

function getEmojiForSentiment(sentiment: SentimentType): string {
  switch (sentiment) {
    case "positivo": return "ğŸ˜Š";
    case "negativo": return "ğŸ˜Ÿ";
    case "neutro": return "ğŸ˜";
  }
}
```

**Ventaja:** TypeScript garantiza que solo se usen valores vÃ¡lidos.

---

### React Patterns Usados:

#### **Functional Components + Hooks**
```typescript
export function TYRChat() {
  const [mensajes, setMensajes] = useState<Message[]>([]);
  const [cargando, setCargando] = useState(false);
  const mensajesEndRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    // Auto-scroll al Ãºltimo mensaje
    mensajesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [mensajes]);

  const enviarMensaje = async (texto: string) => {
    setCargando(true);
    try {
      const response = await fetch(`${API_URL}/chat`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ mensaje: texto })
      });
      const data: ChatResponse = await response.json();
      // Agregar mensaje...
    } catch (error) {
      console.error("Error:", error);
    } finally {
      setCargando(false);
    }
  };

  return (/* JSX */);
}
```

**Patterns:**
- **useState**: Estado local del componente
- **useEffect**: Efectos secundarios (scroll automÃ¡tico)
- **useRef**: Referencias directas al DOM
- **async/await**: Manejo de promesas limpio

---

## 3.3 JSON - ConfiguraciÃ³n Declarativa

### SeparaciÃ³n de CÃ³digo y Datos

#### **config.json** (ConfiguraciÃ³n BERT)
```json
{
  "_name_or_path": "dccuchile/bert-base-spanish-wwm-uncased",
  "architectures": ["BertForSequenceClassification"],
  "num_labels": 9,
  "max_position_embeddings": 512,
  "hidden_size": 768,
  "num_attention_heads": 12,
  "num_hidden_layers": 12
}
```

**Ventaja:** Cambiar configuraciÃ³n sin tocar cÃ³digo Python

#### **label_map.json** (Mapeo Intenciones)
```json
{
  "id2label": {
    "0": "becas_financiamiento",
    "1": "contacto_ubicacion",
    "2": "faq_general",
    "3": "fuera_dominio",
    "4": "horarios_duracion",
    "5": "informacion_carreras",
    "6": "inscripcion_admision",
    "7": "requisitos_ingreso",
    "8": "saludo_despedida"
  }
}
```

**Ventaja:** Agregar nuevas intenciones modificando solo JSON

#### **carreras_itse.json** (Base de Conocimiento)
```json
{
  "big_data": {
    "nombre_completo": "TÃ©cnico Superior en Big Data",
    "escuela": "InnovaciÃ³n Digital",
    "duracion_diurna": "2 aÃ±os 4 meses",
    "creditos": 122,
    "descripcion": "...",
    "campo_ocupacional": [...]
  }
}
```

**Ventaja:** Actualizar contenido sin re-entrenar modelo

---

# 4. DECISIONES TÃ‰CNICAS Y PATRONES (â‰ˆ4 MIN)

## 4.1 Patrones de DiseÃ±o Implementados

### **Singleton Pattern** (Instancia Ãºnica del modelo)
```python
# tyr_simple.py
tyr_instance = None

def get_tyr_instance():
    global tyr_instance
    if tyr_instance is None:
        tyr_instance = TYR(modelo_path="modelo_bert_tyr_4358")
    return tyr_instance
```

**Por quÃ©:** Cargar BERT en memoria es costoso (500MB+). Una sola instancia sirve todas las peticiones.

### **Strategy Pattern** (MÃºltiples estrategias de respuesta)
```python
def _generar_respuesta(self, intencion: str, entidades: dict) -> str:
    estrategias = {
        "informacion_carreras": self._responder_carreras,
        "becas_financiamiento": self._responder_becas,
        "fuera_dominio": self._responder_fuera_dominio
    }
    estrategia = estrategias.get(intencion, self._respuesta_default)
    return estrategia(entidades)
```

**Por quÃ©:** Cada intenciÃ³n puede tener lÃ³gica de respuesta diferente.

### **Factory Pattern** (CreaciÃ³n de entidades)
```python
class NERExtractor:
    def _crear_entidad(self, texto: str, tipo: str, inicio: int, fin: int) -> dict:
        return {
            "texto": texto.lower(),
            "tipo": tipo,
            "inicio": inicio,
            "fin": fin,
            "confianza": self._calcular_confianza(texto, tipo)
        }
```

---

## 4.2 Decisiones ArquitectÃ³nicas Clave

### **Â¿Por quÃ© FastAPI sobre Flask/Django?**

| CaracterÃ­stica | FastAPI | Flask | Django |
|---------------|---------|-------|--------|
| **Performance** | Muy alta (async) | Media | Media |
| **ValidaciÃ³n** | AutomÃ¡tica (Pydantic) | Manual | ORM |
| **DocumentaciÃ³n** | Auto (OpenAPI) | Manual | Manual |
| **Type hints** | Nativo | Opcional | Opcional |
| **Async/await** | SÃ­ | Limitado | Limitado |

**DecisiÃ³n:** FastAPI por performance y validaciÃ³n automÃ¡tica.

### **Â¿Por quÃ© React sobre Vue/Angular?**

- **Ecosistema**: Mayor cantidad de librerÃ­as (jsPDF, html2canvas, etc.)
- **TypeScript support**: Excelente integraciÃ³n
- **Comunidad**: MÃ¡s recursos y ejemplos
- **Hooks**: Simplicidad sobre class components

### **Â¿Por quÃ© NER personalizado sobre SpaCy?**

```python
# SpaCy (librerÃ­a genÃ©rica)
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp("Estudiar Big Data en ITSE")
# Problema: No reconoce "Big Data" ni "ITSE" como entidades

# Mi NER personalizado
ner = NERExtractor()
entidades = ner.extraer_entidades("Estudiar Big Data en ITSE")
# Resultado: CARRERA: big data, ORGANIZACION: itse
```

**Razones:**
1. **Mayor precisiÃ³n**: 95% vs 60-70% de SpaCy genÃ©rico
2. **Zero dependencias**: Puro Python + regex
3. **Compatibilidad**: Python 3.14 tiene conflictos con SpaCy
4. **Control total**: Puedo agregar nuevas entidades fÃ¡cilmente

---

## 4.3 Manejo de Errores y Edge Cases

### **ValidaciÃ³n en mÃºltiples capas**

#### **Capa 1: Frontend (TypeScript)**
```typescript
if (mensaje.trim().length === 0) {
  return; // No enviar mensajes vacÃ­os
}

if (mensaje.length > 500) {
  alert("Mensaje muy largo (mÃ¡x 500 caracteres)");
  return;
}
```

#### **Capa 2: API (Pydantic)**
```python
class ChatRequest(BaseModel):
    mensaje: str = Field(..., min_length=1, max_length=500)

    @validator('mensaje')
    def validar_mensaje(cls, v):
        if not v.strip():
            raise ValueError('Mensaje vacÃ­o')
        return v.strip()
```

#### **Capa 3: LÃ³gica (Python)**
```python
def procesar_consulta(self, mensaje: str) -> Tuple[str, Dict]:
    try:
        # Procesamiento...
    except Exception as e:
        logger.error(f"Error procesando: {e}")
        return (
            "Disculpa, ocurriÃ³ un error. Intenta reformular tu pregunta.",
            {"error": str(e)}
        )
```

### **Fallback a Mock Responses**
```typescript
try {
  const response = await fetch(`${API_URL}/chat`, {...});
  const data = await response.json();
} catch (err) {
  console.warn("Backend no disponible, usando mock");
  const mockData = getMockResponse(mensaje);
  // Continuar con datos de prueba
}
```

**Ventaja:** La app funciona incluso si el backend estÃ¡ caÃ­do (modo demo).

---

## 4.4 Optimizaciones de Performance

### **Carga diferida de modelo**
```python
class TYR:
    def __init__(self):
        # Modelo se carga una vez
        self._cargar_modelo()

    def procesar_consulta(self, mensaje: str):
        # Reutiliza modelo ya cargado
        with torch.no_grad():  # Sin gradientes = mÃ¡s rÃ¡pido
            outputs = self.modelo(**inputs)
```

### **MemoizaciÃ³n de respuestas**
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def _obtener_respuesta_carrera(self, carrera: str) -> str:
    # Cache de respuestas frecuentes
    return self.carreras_itse[carrera]["descripcion"]
```

### **Batch processing (preparado)**
```python
# Actualmente: una consulta a la vez
# Futuro: procesar mÃºltiples consultas en batch
def procesar_batch(self, mensajes: List[str]) -> List[Dict]:
    inputs = self.tokenizer(mensajes, padding=True, truncation=True)
    with torch.no_grad():
        outputs = self.modelo(**inputs)
    return self._parsear_outputs(outputs)
```

---

# 5. DEMOSTRACIÃ“N TÃ‰CNICA (â‰ˆ3 MIN)

## Live Coding: Agregar Nueva IntenciÃ³n

Voy a mostrar cÃ³mo se agrega una nueva intenciÃ³n al sistema en **3 archivos**:

### **Paso 1: Actualizar label_map.json**
```json
{
  "id2label": {
    ...
    "9": "eventos_actividades"  // â† Nueva intenciÃ³n
  }
}
```

### **Paso 2: Agregar respuesta en respuestas_base.json**
```json
{
  "eventos_actividades": {
    "respuesta_base": "El ITSE organiza hackathons, ferias tech...",
    "contexto": ["eventos", "actividades", "hackathon", "feria"]
  }
}
```

### **Paso 3: Re-entrenar modelo (o agregar rule-based)**
```python
# OpciÃ³n rÃ¡pida: detecciÃ³n por keywords
def _clasificar_intencion(self, mensaje: str):
    if any(kw in mensaje for kw in ["evento", "actividad", "hackathon"]):
        return "eventos_actividades", 0.95

    # Sino, usar BERT normal
    return self._clasificar_con_bert(mensaje)
```

**Resultado:** Nueva funcionalidad en minutos sin reescribir todo.

---

## Demo: Pipeline de Inferencia

### **Input del usuario:**
```
"Quiero estudiar Ciberseguridad en el ITSE de Tocumen"
```

### **Paso 1: NormalizaciÃ³n**
```python
# Input: "Quiero estudiar Ciberseguridad en el ITSE de Tocumen"
mensaje_norm = self._normalizar_texto(mensaje)
# Output: "quiero estudiar ciberseguridad en el itse de tocumen"
```

### **Paso 2: TokenizaciÃ³n**
```python
tokens = self.tokenizer(mensaje_norm, return_tensors="pt")
# Output: {
#   'input_ids': tensor([[101, 2543, 9876, ...]]),
#   'attention_mask': tensor([[1, 1, 1, ...]])
# }
```

### **Paso 3: ClasificaciÃ³n**
```python
with torch.no_grad():
    outputs = self.modelo(**tokens)
    logits = outputs.logits
    prediccion = torch.argmax(logits, dim=1).item()
# Output: prediccion = 5 â†’ "informacion_carreras"
```

### **Paso 4: NER**
```python
entidades = self.ner.extraer_entidades(mensaje)
# Output: [
#   {"tipo": "CARRERA", "texto": "ciberseguridad"},
#   {"tipo": "ORGANIZACION", "texto": "itse"},
#   {"tipo": "UBICACION", "texto": "tocumen"}
# ]
```

### **Paso 5: Respuesta**
```python
respuesta_final = self._generar_respuesta("informacion_carreras", entidades)
# Output: "La T.S. en Ciberseguridad del ITSE es una carrera..."
```

---

# 6. INTEGRACIÃ“N Y DEPLOY (â‰ˆ2 MIN)

## 6.1 Testing Automatizado

### **Unit Tests (pytest)**
```python
# tests/test_chatbot.py
def test_clasificacion_intencion():
    tyr = TYR()
    intencion, conf = tyr._clasificar_intencion("Quiero informaciÃ³n sobre becas")
    assert intencion == "becas_financiamiento"
    assert conf > 0.90

# tests/test_ner.py
def test_extraccion_carrera():
    ner = NERExtractor()
    entidades = ner.extraer_entidades("Big Data")
    assert entidades[0]["tipo"] == "CARRERA"
    assert entidades[0]["texto"] == "big data"
```

**Resultado:** 80 tests, 91% coverage

### **Integration Tests**
```python
# tests/test_api.py
from fastapi.testclient import TestClient

def test_chat_endpoint():
    client = TestClient(app)
    response = client.post("/chat", json={"mensaje": "Hola"})
    assert response.status_code == 200
    assert "respuesta" in response.json()
```

---

## 6.2 Deploy y ProducciÃ³n

### **Backend Deploy (Render/Railway)**
```bash
# Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### **Frontend Deploy (Vercel/Netlify)**
```bash
# Build
npm run build

# Deploy
vercel deploy --prod
```

### **Environment Variables**
```bash
# Backend
MODELO_PATH=./modelo_bert_tyr_4358
MAX_LENGTH=128
DEVICE=cpu

# Frontend
VITE_API_URL=https://api-tyr.onrender.com
```

---

## 6.3 Versionado y Git

```bash
TYR/
â”œâ”€â”€ .git/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ tyr_chatbot.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ package.json
â””â”€â”€ modelo_bert_tyr_4358/  # .gitignore (muy pesado)
```

**Commits importantes:**
```
feat: add sentiment analysis visualization
feat: add Named Entity Recognition module
fix: improve BERT classification accuracy
docs: add comprehensive API documentation
```

---

# 7. CONCLUSIONES (â‰ˆ1 MIN)

## Logros TÃ©cnicos

### **Arquitectura Multi-Lenguaje**
- âœ… **Python** para backend y ML (1,400+ lÃ­neas)
- âœ… **TypeScript** para frontend type-safe (1,100+ lÃ­neas)
- âœ… **JSON** para configuraciÃ³n declarativa

### **Patrones de DiseÃ±o**
- âœ… **Singleton**: Instancia Ãºnica del modelo
- âœ… **Strategy**: MÃºltiples estrategias de respuesta
- âœ… **Factory**: CreaciÃ³n de entidades NER

### **IntegraciÃ³n de LibrerÃ­as**
- âœ… **PyTorch + Transformers**: Modelo BERT en producciÃ³n
- âœ… **FastAPI + Pydantic**: API REST con validaciÃ³n automÃ¡tica
- âœ… **React + Hooks**: UI moderna y reactiva

### **Calidad de CÃ³digo**
- âœ… **80 tests automatizados** con 91% coverage
- âœ… **Type safety** en frontend y backend
- âœ… **Manejo de errores** en mÃºltiples capas
- âœ… **DocumentaciÃ³n** OpenAPI automÃ¡tica

### **Performance**
- âœ… **98.93% accuracy** en clasificaciÃ³n
- âœ… **95% precisiÃ³n** en NER personalizado
- âœ… **<500ms** tiempo de respuesta promedio
- âœ… **Singleton pattern** para eficiencia de memoria

---

## Aprendizajes Clave

1. **Elegir el lenguaje correcto para cada tarea**
   - Python para IA/ML
   - TypeScript para frontend robusto
   - JSON para configuraciÃ³n

2. **Arquitectura en capas facilita mantenimiento**
   - Frontend, API y ML independientes
   - Cada capa testeada aisladamente

3. **Type safety previene errores**
   - TypeScript + Pydantic detectan problemas antes de runtime
   - Interfaces claras entre componentes

4. **Testing automatizado da confianza**
   - 80 tests garantizan estabilidad
   - Refactoring seguro

5. **Patrones de diseÃ±o resuelven problemas reales**
   - Singleton para modelos pesados
   - Strategy para mÃºltiples comportamientos

---

## TYR en NÃºmeros

```
ğŸ“Š MÃ©tricas de CÃ³digo:
- 2,500+ lÃ­neas de cÃ³digo de producciÃ³n
- 3 lenguajes principales
- 80 tests automatizados
- 91% code coverage

ğŸ¤– MÃ©tricas de IA:
- 98.93% accuracy BERT
- 95% precisiÃ³n NER
- 9 intenciones clasificadas
- 6 tipos de entidades

ğŸ—ï¸ Arquitectura:
- 3 capas (UI, API, ML)
- 5 patrones de diseÃ±o
- 10+ librerÃ­as integradas
- REST API con OpenAPI
```

---

## Preguntas que puedo responder:

- Â¿Por quÃ© elegiste Python sobre otros lenguajes para IA?
- Â¿CÃ³mo manejas la memoria con modelos de 500MB+?
- Â¿QuÃ© patrones de diseÃ±o usarÃ­as para escalar a millones de usuarios?
- Â¿CÃ³mo implementarÃ­as streaming de respuestas (como ChatGPT)?
- Â¿QuÃ© ventajas tiene FastAPI sobre Flask para APIs de ML?

**Gracias por su atenciÃ³n. Quedo abierto a preguntas tÃ©cnicas.**

---

**Archivos de referencia:**
- CÃ³digo fuente: [GitHub - TYR](https://github.com/EiTinchoZ/TYR)
- DocumentaciÃ³n API: `http://localhost:8000/docs` (OpenAPI)
- Tests: `pytest -v --cov`
